{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use pyctcdecode when working with a SpeechBrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpeechBrain will release their own n-gram scorer, this fall, along with their already existing TransformerLM rescorer. This notebook gives an example of how to use SpeechBrain models with pyctcdecode and its features. There are many models implementations in SpeechBrain. This notebook gives a walkthrough for the following models:\n",
    "* EnoderASR (shared by @Moumeneb1 at [link](https://github.com/speechbrain/speechbrain/issues/1467#issuecomment-1184340981))\n",
    "* EncoderDecoderASR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.5 ('base')' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Install SpeechBrain\n",
    "! pip install speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single audio file\n",
    "! wget https://dldata-public.s3.us-east-2.amazonaws.com/1919-142785-0028.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastwer\n",
    "import torch, os, torchaudio\n",
    "\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from speechbrain.pretrained import EncoderASR\n",
    "from speechbrain.pretrained import EncoderDecoderASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snippet as shared by @Moumeneb1 at [link](https://github.com/speechbrain/speechbrain/issues/1467#issuecomment-1184340981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_encoder_model():\n",
    "    model = EncoderASR.from_hparams(source=\"speechbrain/asr-wav2vec2-commonvoice-fr\", savedir=\"pretrained_models/asr-wav2vec2-commonvoice-fr\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_encoderasr(file_path, arpa_path):\n",
    "\n",
    "    #load model\n",
    "    asr_model = load_encoder_model()\n",
    "\n",
    "    #encode the wav with wav2vec2+vanilla_enc+ctc head \n",
    "    #load audio\n",
    "    audio, sr  = torchaudio.load('/content/speechbrain/example1.wav')\n",
    "    rel_length = torch.tensor([1.0])\n",
    "    encoder_out  = asr_model.encode_batch(audio,rel_length)\n",
    "\n",
    "    #get the labels from the used tokenizer\n",
    "    labels  = [asr_model.tokenizer.id_to_piece(id).lower() for id in range(asr_model.tokenizer.get_piece_size())]\n",
    "\n",
    "    labels[1]=''\n",
    "    labels[0] = '<pad>'\n",
    "\n",
    "\n",
    "    decoder = build_ctcdecoder(labels,\n",
    "        \"model.arpa\",\n",
    "        alpha=0.6)\n",
    "\n",
    "    transcript = decoder.decode(encoder_out[0].cpu().numpy())\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoderASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either use an existing checkpoint directory or download from \n",
    "# https://drive.google.com/drive/folders/15pjw9NnGIyAP1P_z1Vn8xtNYLQH32m3V (SpeechBrain author uploads)\n",
    "\n",
    "def load_model():\n",
    "    model = EncoderDecoderASR.from_hparams(\n",
    "        source=\"/root/workspace/results/CRDNN_BPE_960h_LM/2602/save\", savedir=\"pretrained_model\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_mem(model, batch_size, device):\n",
    "    \"\"\"Needed to reset the memory during beamsearch.\"\"\"\n",
    "    hs = None\n",
    "    model.mods.decoder.dec.attn.reset()\n",
    "    c = torch.zeros(batch_size,  model.mods.decoder.dec.attn_dim, device=device)\n",
    "    return hs, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def decode_encoderdecoderasr(sound_file, arpa_path):\n",
    "    model = load_model()\n",
    "    device = model.device\n",
    "    wav = model.load_audio(sound_file)\n",
    "    wavs = wav.unsqueeze(0).to(device)\n",
    "    wav_lens = torch.tensor([1.0])\n",
    "    wav_lens = wav_lens.to(device)\n",
    "    encoder_out = model.mods.encoder(wavs, wav_lens)\n",
    "    enc_lens = torch.round(encoder_out.shape[1] * wav_lens).int()\n",
    "    device = encoder_out.device\n",
    "    batch_size = encoder_out.shape[0]\n",
    "    memory = reset_mem(model, batch_size, device=device)\n",
    "\n",
    "    # Using bos as the first input\n",
    "    inp_tokens = (encoder_out.new_zeros(batch_size).fill_(model.mods.decoder.bos_index).long())\n",
    "    log_probs_lst = []\n",
    "    max_decode_steps = int(encoder_out.shape[1] * model.mods.decoder.max_decode_ratio)\n",
    "    for t in range(max_decode_steps):\n",
    "        log_probs, memory, _ = model.mods.decoder.forward_step(\n",
    "            inp_tokens, memory, encoder_out, enc_lens)   \n",
    "        log_probs_lst.append(log_probs)\n",
    "        inp_tokens = log_probs.argmax(dim=-1)\n",
    "    log_probs = torch.stack(log_probs_lst, dim=1)\n",
    "    squeezed_log_probs = torch.squeeze(log_probs, dim =0)\n",
    "\n",
    "    labels  = [model.tokenizer.id_to_piece(id).upper() for id in range(model.tokenizer.get_piece_size())]\n",
    "    labels[0] ='<pad>'\n",
    "    labels[1]='' # Add more customizations here based on bos and eos index from the hyperparameter.yaml file\n",
    "\n",
    "    decoder = build_ctcdecoder(labels, arpa_path)\n",
    "    transcript = decoder.decode(squeezed_log_probs.numpy(), beam_width=50)\n",
    "   \n",
    "    return transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    file_path = \"1919-142785-0028.wav\"\n",
    "    arpa_path = \"/root/workspace/lms/3-gram.pruned.3e-7.arpa\"\n",
    "    \n",
    "    ground_truth, transcript =[], []\n",
    "    ground_truth.append(\"\")\n",
    "    transcript.append(decode_encoderdecoderasr(file_path, arpa_path))\n",
    "    transcript.append(decode_encoderasr(file_path, arpa_path))\n",
    "\n",
    "\n",
    "    print(fastwer.score(ground_truth, transcript[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7430d68fc02621aad1ca3098f8d3af5b679033b6b1df19181c62f470e45ed255"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
