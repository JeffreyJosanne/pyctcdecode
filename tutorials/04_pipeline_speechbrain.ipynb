{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use pyctcdecode when working with a SpeechBrain model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpeechBrain will release their own n-gram scorer, this fall, along with their already existing TransformerLM scorer. This notebook gives an example of how to use SpeechBrain models with pyctcdecode and its features. There are many models implementations in SoeechBrain. This notebook gives a walkthrough for the follwoing models:\n",
    "* EnoderASR (shared by @Moumeneb1 at [link](https://github.com/speechbrain/speechbrain/issues/1467#issuecomment-1184340981))\n",
    "* EncoderDecoderASR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install SpeechBrain\n",
    "! pip install speechbrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a single audio file\n",
    "!wget https://dldata-public.s3.us-east-2.amazonaws.com/1919-142785-0028.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastwer\n",
    "import torch, os\n",
    "\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from speechbrain.pretrained import EncoderDecoderASR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EncoderDecoderASR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Either use an existing checkpoint directory or download from \n",
    "# https://drive.google.com/drive/folders/15pjw9NnGIyAP1P_z1Vn8xtNYLQH32m3V (SpeechBrain author uploads)\n",
    "\n",
    "def load_model():\n",
    "    model = EncoderDecoderASR.from_hparams(\n",
    "        source=\"/root/workspace/results/CRDNN_BPE_960h_LM/2602/save\", savedir=\"pretrained_model\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_mem(model, batch_size, device):\n",
    "    \"\"\"Needed to reset the memory during beamsearch.\"\"\"\n",
    "    hs = None\n",
    "    model.mods.decoder.dec.attn.reset()\n",
    "    c = torch.zeros(batch_size,  model.mods.decoder.dec.attn_dim, device=device)\n",
    "    return hs, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def main(sound_file, arpa_path):\n",
    "    model = load_model()\n",
    "\n",
    "    device = model.device\n",
    "\n",
    "    wav = model.load_audio(sound_file)\n",
    "    # wav is a 1-d tensor, e.g., [52173]\n",
    "    wavs = wav.unsqueeze(0).to(device)\n",
    "    # wavs is a 2-d tensor, e.g., [1, 52173]\n",
    "\n",
    "    wav_lens = torch.tensor([1.0])\n",
    "    wav_lens = wav_lens.to(device)\n",
    "\n",
    "    encoder_out = model.mods.encoder(wavs, wav_lens)\n",
    "    enc_lens = torch.round(encoder_out.shape[1] * wav_lens).int()\n",
    "    device = encoder_out.device\n",
    "    batch_size = encoder_out.shape[0]\n",
    "\n",
    "    memory = reset_mem(model, batch_size, device=device)\n",
    "\n",
    "    # Using bos as the first input\n",
    "    inp_tokens = (\n",
    "        encoder_out.new_zeros(batch_size).fill_(model.mods.decoder.bos_index).long()\n",
    "    )\n",
    "\n",
    "    log_probs_lst = []\n",
    "    max_decode_steps = int(encoder_out.shape[1] * model.mods.decoder.max_decode_ratio)\n",
    "\n",
    "    for t in range(max_decode_steps):\n",
    "        log_probs, memory, _ = model.mods.decoder.forward_step(\n",
    "            inp_tokens, memory, encoder_out, enc_lens)   \n",
    "        log_probs_lst.append(log_probs)\n",
    "        inp_tokens = log_probs.argmax(dim=-1)\n",
    "\n",
    "    log_probs = torch.stack(log_probs_lst, dim=1)\n",
    "    squeezed_log_probs = torch.squeeze(log_probs, dim =0)\n",
    "\n",
    "    labels  = [model.tokenizer.id_to_piece(id).upper() for id in range(model.tokenizer.get_piece_size())]\n",
    "\n",
    "    labels[0] ='<pad>'\n",
    "    labels[1]=''\n",
    "    # Add more customizations here based on bos and eos index from the hyperparameter.yaml file\n",
    "\n",
    "    decoder = build_ctcdecoder(labels, arpa_path)\n",
    "    text_w_lm = decoder.decode(squeezed_log_probs.numpy(), beam_width=50)\n",
    "   \n",
    "    return text_w_lm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    file_path = \"1919-142785-0028.wav\"\n",
    "    arpa_path = \"/root/workspace/lms/3-gram.pruned.3e-7.arpa\"\n",
    "    \n",
    "    ground_truth, transcript =[], []\n",
    "    ground_truth.append(\"\")\n",
    "    transcript.append(main(file_path, arpa_path))\n",
    "\n",
    "    print(fastwer.score(ground_truth, transcript))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
